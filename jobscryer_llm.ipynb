{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b47a806",
   "metadata": {},
   "source": [
    "Using the data webscraped with the \"jobscryer\" repo here:\n",
    "\n",
    "https://github.com/tyleryou/jobscryer\n",
    "\n",
    "The goal of the following script is to leverage a LLM to summarize each job description, then find a pattern between the summaries. This is all purely academic and for self-training purposes.\n",
    "\n",
    "The following script utilized ChatGPT 4 to build a general model template, with manually added specification on what each element of the model does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b10c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.environ.get('jobscryer_data_path')\n",
    "\n",
    "df_init = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62944de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_init[df_init['description'] != '{}'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d2e839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6149b5cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing description: 100%|████████████████████████████████████████████████████| 795/795 [00:00<00:00, 1647.45desc/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing 795 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing:   8%|█████                                                           | 2/25 [07:33<1:26:53, 226.67s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Telling the model not to compute or store gradients, saving memory and speeding up prediction\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Generate summary\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     summary_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     82\u001b[0m         b_input_ids, \n\u001b[0;32m     83\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,    \u001b[38;5;66;03m# Number of beams for beam search\u001b[39;00m\n\u001b[0;32m     84\u001b[0m         min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# Minimum length of the summary\u001b[39;00m\n\u001b[0;32m     85\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, \u001b[38;5;66;03m# Maximum length of the summary\u001b[39;00m\n\u001b[0;32m     86\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Stop generation when all beam hypotheses reached EOS\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mb_input_mask\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Decode the summary tokens and convert to strings\u001b[39;00m\n\u001b[0;32m     91\u001b[0m summaries\u001b[38;5;241m.\u001b[39mextend([tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m summary \u001b[38;5;129;01min\u001b[39;00m summary_ids])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1486\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1480\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1481\u001b[0m         )\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1484\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m   1487\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[0;32m   1488\u001b[0m     )\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:655\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    653\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 655\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:871\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    864\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    865\u001b[0m             create_custom_forward(encoder_layer),\n\u001b[0;32m    866\u001b[0m             hidden_states,\n\u001b[0;32m    867\u001b[0m             attention_mask,\n\u001b[0;32m    868\u001b[0m             (head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 871\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m encoder_layer(\n\u001b[0;32m    872\u001b[0m             hidden_states,\n\u001b[0;32m    873\u001b[0m             attention_mask,\n\u001b[0;32m    874\u001b[0m             layer_head_mask\u001b[38;5;241m=\u001b[39m(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    875\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    876\u001b[0m         )\n\u001b[0;32m    878\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained model tokenizer (for example, facebook's BART)\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "                        # -- Tokenize and encode sequences in the \"description\" column -- #\n",
    "# Tokenizing is just separating the sentences (sequences) into individual words. \n",
    "# Then the tokenizer assigns token ids (pre-defined in the BART-large model pre-training process) to be encoded. \n",
    "# Encoding is the process of converting each word (token) into vectors.\n",
    "# In classical NLP such as classifiers, we encode only the tokens. But in LLM models like BART \n",
    "# (Bidirectional Auto Regression from Transformers), the vectors are generated with respect to the tokens to the left and right\n",
    "# of the token being encoded. The token limit per sequence is 1024 for BART (so the amount of words allowed per input sentence).\n",
    "# Which is the \"max_length\" in the encoded_plus method below. We pad the tokenized sequence (add 0's) to the vector so that \n",
    "# all encoded elements are the same length, which is important when doing tensor maths. Each encoded vector is \n",
    "# length 1024 elements, each element 0 <= x >= 1. Tensor size will be based on 3 values, n = rows in dataset, \n",
    "# the max sequence length chosen (1024 for this model), and the embedding dimension which was chosen during the original\n",
    "# training of the model. In summary: tensor size = (n, max_sequence_length, embedding_dimension)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for desc in tqdm(df['description'], desc=\"Tokenizing description\", unit=\"desc\"):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        desc,                        # Sentence to encode.\n",
    "        add_special_tokens=True,     # Add '[CLS]' and '[SEP]', these are the beginning and end of the sentence\n",
    "        max_length=1024,             # Max characters allowed for BART-large is 1024\n",
    "        padding='max_length',        # Pad all to max_length (required)\n",
    "        return_attention_mask=True,  # Construct attention masks to use later.\n",
    "        return_tensors='pt',         # Return pytorch tensors to use later.\n",
    "        truncation=True              # Truncate longer messages over 1024 (shortens sentence to max allowed).\n",
    "    )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Set the batch size.\n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader. TensorDataset builds out a single tensor out of the input IDs and the attention mask.\n",
    "# The attention mask tells the model what token to pay attention to and which to ignore. AKA the \"attention\" in transformers.\n",
    "# The sample selects a subset of data points (or samples) from the larger dataset. \n",
    "# This forms a mini-batch, which is set to 32 above. Batches are computed collectively as opposed to sequentially \n",
    "# (one at a time) like more traditional NLP models like a Recurrent Neural Network (RNN).\n",
    "# The DataLoader then builds a new PyTorch class that wraps the tensor built from the input data and attention masks\n",
    "# that acts as a pipeline for shuffling, batching and parallel loading.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "# Prediction on test set\n",
    "print('Summarizing {:,} documents...'.format(len(input_ids)))\n",
    "\n",
    "# Load the pre-trained BART model for sequence-to-sequence summarization\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Put model in evaluation mode. This ensures the model isn't really trained (as it is already pre-trained) \n",
    "# but instead points it towards predicting values.\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "summaries = []\n",
    "\n",
    "# Summarize \n",
    "for batch in tqdm(prediction_dataloader, desc=\"Summarizing\", unit=\"batch\"):\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Generate summary\n",
    "        summary_ids = model.generate(\n",
    "            b_input_ids, \n",
    "            num_beams=4,    # Number of beams for beam search\n",
    "            min_length=30,  # Minimum length of the summary\n",
    "            max_length=200, # Maximum length of the summary\n",
    "            early_stopping=True,  # Stop generation when all beam hypotheses reached EOS\n",
    "            attention_mask=b_input_mask\n",
    "        )\n",
    "\n",
    "    # Decode the summary tokens and convert to strings\n",
    "    summaries.extend([tokenizer.decode(summary, skip_special_tokens=True, clean_up_tokenization_spaces=True) for summary in summary_ids])\n",
    "\n",
    "print('    DONE.')\n",
    "\n",
    "# Now the 'summaries' list contains the generated summaries for each input document\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
