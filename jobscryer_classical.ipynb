{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7490ed0f",
   "metadata": {},
   "source": [
    "The following is a quick research on data from \"ai-jobs.net\", a popular data science job listing site. \n",
    "I use simple \"classical\" (non-LLM) machine learning methods to glean insights on what the current trending tools are in the data science field.\n",
    "All data was obtained via webscraping under the \"jobscryer\" repo https://github.com/tyleryou/jobscryer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a976eaa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tyler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tyler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79f86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.environ.get('jobscryer_data_path')\n",
    "df_init = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e4d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_init\n",
    "\n",
    "# Fill NaN values with an empty string, null values throw errors when trying to stem or lemmatize the words.\n",
    "\n",
    "df['description'].fillna('', inplace=True)\n",
    "\n",
    "# Perform string operations to eliminate non-alphebetical text. Non-alphebetical text is \n",
    "# useless when gleaning insights from the text. In fact, it'll probably break the model.\n",
    "df['description'] = df['description'].str.replace(r'[^a-zA-Z ]', '', regex=True).str.lower()\n",
    "\n",
    "# Initialize Wordnet Lemmatizer. Lemmatizer is similar to a stemmer (PorterStemmer through nltk)\n",
    "# in that they both cut the words down to their base versions. Prefixes and conjugates are removed, for example:\n",
    "# 'runs' and 'running' become 'run.' This is the stem (or base) of the word.\n",
    "# The difference between the two is that stemming specifically removes prefixes and suffixes without converting the word.\n",
    "# Lemmatizing actually transforms the word into the base form, known as the 'lemma.'\n",
    "# Stemming is faster and simpler, but some words are chopped off, for example; 'experiencing' becomes 'experienc'\n",
    "# Lemmatizing is good for smaller datasets and better accuracy for the words original form.\n",
    "# Since the dataset we're using is smaller, we can juse lemmatize it.\n",
    "\n",
    "lm = WordNetLemmatizer() \n",
    "\n",
    "# Create a set of stopwords with non-alphabetic characters removed. Stopwords is a popular library full of \n",
    "# words that are generally useless for NLP, such as 'a, i, the, has.' These words give no insight towards the \n",
    "# what the actual meaning of the text is. However, 'not' is included in stopwords, which can actually give an entirely\n",
    "# different meaning based on its presence, aka 'not good' vs 'good.'\n",
    "\n",
    "word_set = set([re.sub(r'[^a-zA-Z]', '', word) for word in stopwords.words('english')])\n",
    "word_set.remove('not') # taking out \"not\" because this is a useful word\n",
    "\n",
    "def check_words(row): \n",
    "    lemmatized_words = [lm.lemmatize(word) for word in row.split() if word not in word_set]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply the function to the 'description' column because this is what we're using to ultimately glean insights.\n",
    "df['description'] = df['description'].apply(check_words)x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3b3b22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prep data for model ingestion ###\n",
    "# CountVectorizer turns each string into a binary column (what we actually feed into the model.)\n",
    "# X will have the length of original dataset rows with n columns (n = max_features)\n",
    "# Each column is one word. Setting max_features = 1500 will use 1500 unique words (the parameter automatically takes unique words.)  \n",
    "# This is a finetuning parameter, as too many words risks overfitting. If the model is overfitting, it will simply guess\n",
    "# the training data instead of using extrapolation. However, too few words will lead to underfitting, \n",
    "# which would lead to inaccurate predictions. Larger values of max_features also requires more computational power.\n",
    "# Size of dataset can give an indication of how large the magnitude of max_features should be. However, a good practice\n",
    "# is to start with 1000 - 5000 words and finetune based on the output of the model.\n",
    "\n",
    "\n",
    "# A corpus refers to an organized block of text. This is what we'll fit to our model.\n",
    "\n",
    "corpus = df['description'].values\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 2000) \n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "#y = df['salary'].values # For now we won't be using a Y value because we aren't actually trying to predict any values\n",
    "# at this point, we just want to find the most frequent words and most frequent phrases/word distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1a7f2a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words:\n",
      "data: 45807\n",
      "experience: 26604\n",
      "team: 13932\n",
      "business: 10614\n",
      "work: 10481\n",
      "skill: 8710\n",
      "de: 8103\n",
      "year: 7739\n",
      "development: 7223\n",
      "model: 7178\n"
     ]
    }
   ],
   "source": [
    "# The top 10 most occurring words here, which makes sense. AI jobs will need data, experience, etc. None of this is surprising,\n",
    "# or more importantly useful. The top words are simply buzzwords, so if we look at the less frequent words (rows 30 and up\n",
    "# in the sorted words), we see more interesting items.\n",
    "\n",
    "words = cv.get_feature_names_out()\n",
    "\n",
    "word_frequencies = X.sum(axis=0)\n",
    "\n",
    "word_freq_dict = {word: freq for word, freq in zip(words, word_frequencies.flat)}\n",
    "\n",
    "sorted_words = sorted(word_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Most frequent words:\")\n",
    "for word, freq in sorted_words[:10]:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "55b9ec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words:\n",
      "including: 4911\n",
      "python: 4599\n",
      "machine: 4577\n",
      "sql: 4468\n",
      "develop: 4358\n",
      "degree: 4174\n",
      "new: 4095\n",
      "environment: 4094\n",
      "using: 3989\n",
      "platform: 3961\n"
     ]
    }
   ],
   "source": [
    "# More useful but not surprising, Python is the most frequently sought after skill in data jobs. SQL is very narrowly\n",
    "# behind it however, which also makes sense as SQL dominates the analytics and engineering domains. \n",
    "# Machine is also present, which is probably \"machine learning.\" It seems most jobs use machine learning.\n",
    "\n",
    "print(\"Most frequent words:\")\n",
    "for word, freq in sorted_words[30:40]:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "39df0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most frequent n-grams:\n",
      "machine learning: 4366\n",
      "year experience: 3357\n",
      "data science: 2712\n",
      "computer science: 2500\n",
      "experience working: 2215\n",
      "communication skill: 2064\n",
      "experience data: 1864\n",
      "best practice: 1762\n",
      "data pipeline: 1633\n",
      "bachelor degree: 1626\n",
      "data analysis: 1527\n",
      "related field: 1425\n",
      "data engineering: 1422\n",
      "data quality: 1383\n",
      "skill ability: 1327\n",
      "data analytics: 1303\n",
      "big data: 1262\n",
      "power bi: 1215\n",
      "ability work: 1136\n",
      "team member: 1101\n",
      "data management: 1051\n",
      "degree computer: 1047\n",
      "data visualization: 1022\n",
      "data model: 1017\n",
      "data scientist: 1008\n",
      "degree computer science: 994\n",
      "data set: 874\n",
      "data warehouse: 872\n",
      "master degree: 854\n",
      "programming language: 854\n",
      "experience building: 851\n",
      "deep learning: 817\n",
      "crossfunctional team: 806\n",
      "learning model: 805\n",
      "handson experience: 789\n",
      "data source: 769\n",
      "data governance: 741\n",
      "software development: 740\n",
      "business intelligence: 729\n",
      "excellent communication: 728\n",
      "track record: 722\n",
      "work closely: 706\n",
      "data engineer: 693\n",
      "machine learning model: 690\n",
      "work experience: 688\n",
      "data modeling: 672\n",
      "data platform: 669\n",
      "data processing: 666\n",
      "minimum year: 659\n",
      "least year: 651\n",
      "problemsolving skill: 648\n",
      "experience developing: 610\n",
      "de donne: 609\n",
      "business requirement: 607\n",
      "attention detail: 603\n",
      "experience using: 585\n",
      "project management: 580\n",
      "software engineering: 578\n",
      "data architecture: 563\n",
      "complex data: 558\n",
      "internal external: 541\n",
      "parental leave: 540\n",
      "written verbal: 540\n",
      "visualization tool: 538\n",
      "engineering team: 530\n",
      "cloud platform: 529\n",
      "year experience data: 523\n",
      "analytical skill: 519\n",
      "dental vision: 513\n",
      "ensure data: 504\n",
      "proven experience: 502\n",
      "bachelor degree computer: 496\n",
      "data warehousing: 496\n",
      "business problem: 494\n",
      "business need: 492\n",
      "strong analytical: 485\n",
      "strong communication: 483\n",
      "year relevant: 480\n",
      "data lake: 470\n",
      "bachelor degree computer science: 468\n",
      "data integration: 465\n",
      "understanding data: 461\n",
      "work independently: 461\n",
      "team player: 459\n",
      "science engineering: 458\n",
      "tool like: 455\n",
      "develop maintain: 454\n",
      "field year: 453\n",
      "generative ai: 451\n",
      "science related: 451\n",
      "data product: 450\n",
      "design implement: 449\n",
      "data analyst: 447\n",
      "etc experience: 447\n",
      "design develop: 443\n",
      "new technology: 441\n",
      "experience year: 440\n",
      "working knowledge: 438\n",
      "communication skill ability: 435\n",
      "use case: 434\n"
     ]
    }
   ],
   "source": [
    "# Next we'll use an ngram_range parameter in CountVectorizer that will generate n-grams (sequences of n tokens) instead\n",
    "# of single words. Then we'll analyze the distribution of these n-grams to identify frequent phrases or combinations of words.\n",
    "\n",
    "# The ngram_range gives the range of words you want to include in your phrases. Another parameter that needs to be hypertuned\n",
    "# similar to max_features. \n",
    "vectorizer_ngrams = CountVectorizer(ngram_range=(2, 5))\n",
    "\n",
    "X_ngrams = vectorizer_ngrams.fit_transform(corpus)\n",
    "\n",
    "ngram_features = vectorizer_ngrams.get_feature_names_out()\n",
    "\n",
    "ngram_frequencies = X_ngrams.sum(axis=0)\n",
    "\n",
    "ngram_freq_dict = {ngram: freq for ngram, freq in zip(ngram_features, ngram_frequencies.flat)}\n",
    "\n",
    "sorted_ngrams = sorted(ngram_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Some interesting items here. Most commonly sought after degree is computer science (as opposed to data science.) \n",
    "# Also, a bachelor's degree is mentioned more than a master's degree. \"Bachelor degree\" occurs 1626 times while\n",
    "# \"Master degree\" occurs 854 times. Usually, we also see master's degrees falling under \"preferred\" than \"required.\"\n",
    "# I'm considering pursuing a master's degree, so seeing this gives an interesting perspective.\n",
    "# It's important to consider the actual jobs being shown here, and also not all jobs are the same. Some analyst positions\n",
    "# will have an abundance of engineering, and some data engineering jobs will have quite a bit of analysis work.\n",
    "# This data pertains to all job categories listed on the site.\n",
    "#\n",
    "#\n",
    "# Understandably, machine learning was the most mentioned phrase. However, we can spot some key phrases here such as:\n",
    "# Data engineer at 1422, data analytics at 1303, data scientist at 1008. We can also see the lemmatizer didn't always do a \n",
    "# great job, as \"data engineering\" and \"data engineer\" are present.\n",
    "#\n",
    "# It may be useful to pull the actual job titles to see how many jobs there are per category instead of parsing it from\n",
    "# the description of the job. I ultimately cared more about the descriptions than the titles for this project.\n",
    "#\n",
    "# I finetuned the vectorizer (CountVectorizer above) with different ranges,\n",
    "# but an ngram_range of (2, 5) yielded the most relevant results.\n",
    "#\n",
    "# Next step may be to go back and grab all text after \"Qualifications\"\n",
    "# then predict salary ranges based on jobs, experience, region, and degree level\n",
    "\n",
    "print(\"\\nMost frequent n-grams:\")\n",
    "for ngram, freq in sorted_ngrams[:100]:\n",
    "    print(f\"{ngram}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
